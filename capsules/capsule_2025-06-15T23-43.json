{
  "context": "Here is an activity summary of the user's session:\n- [2025-06-15T23:11:56.096369] Opened `MCP_A2A - PowerPoint` using `POWERPNT.EXE`\n- [2025-06-15T23:12:06.100758] Opened `SPEECH.docx - Word` using `WINWORD.EXE`\n\n\n--- MCP_A2A.pptx ---\nMODEL CONTEXT PROTOCOL \u000b&\u000b GOOGLE A2A\u000b(WEEK 1)\nVARUN DAGA (IN) (Manager, Advisory)\n\nSIDDHARTH M(IN) (Intern, Advisory)\nTrained on vast amounts of general knowledge\nSTRENGTHS AND CURRENT USE CASES OF LLMS\nLIMITATIONS OF LLMS IN BUSINESS CONTEXTS\nCURRENT LIMITATIONS\nNEED FOR INTEGRATION\nOVERCOMING LLM SHORTCOMINGS USING AUGMENTATION (RAG)\nCHALLENGES IN PROVIDING CONTEXT AND INTEGRATION\nEvery API required custom integration logic.\nNo unified format for LLMs to   call tools or interpret responses\nLLMS lacked mechanisms to discover and understand available tools.\nLLMS lacked the ability to autonomously choose actions.\nScattered tools outputs, user prompts and intermediate reasoning.\nStateless agents.\nEvery integration was a one-off project.\nManual building of prompt templates and code integration.\nStandardises interfacing with external tools and data sources.\nMCP provides structured, semantically rich business data for grounding LLMs\nEliminates custom integrations between AI models and different systems.\nAids quicker development\nEnable automated  and structured integration of context\nDynamic discovery and utilization of available tools and resources,\nPre-MCP approach\nPost-MCP approach\nComponents of MCP\nCUSTOMER VALUE PROPOSITION\n\n\n\n\n\n\n\n\nMCP connects siloed HR tools into a unified backend for easier employee management.\nFinance teams can use MCP to centralize financial data ingestion, automate forecasting, and optimize cloud spend through AI agents.\nEnterprises use MCP to centralize policy enforcement, automate audit logging across multiple systems.\n\nMCP\u2019s granular activity logs and version control make it easy to trace AI-generated outputs back to their data sources\nMCP-powered chatbots\u00a0can automate common support tasks by integrating with CRMs and other support platforms. \nHospitals can use MCP-powered systems to stream patient data into AI Agents that can autonomously schedule appointments and retrieve diagnostic data.\nAllows developers to build once and connect to many data sources\nBusinesses use MCP to build agentic frameworks where AI systems pull context from various enterprise systems and leveraging tools and APIs as needed.\nConnecting Disparate Systems and Resources\nEnable Real-Time, Context-Aware AI Agents\nAutomate Compliance, Governance, and Audit\nAccelerating Integration\nAnd Support Agentic Workflows\nEnables multiple specialized agents to work together in a coordinated flow.\nEach agents can focus on a specific task, creating a powerful multi-agent system.\nDevelopers can use YAML-based configuration to define workflows, goals, tools, and agents. \nSimplifies development\nScripted multi-agent workflow, where each agent can call tools independently for its own unique role.\nDynamic discovery and utilization of available tools and resources,\nThe client and remote agents communicate through the A2A protocol.\nEach remote agents comprises LLMs and their associated tools which are accessed through MCP.\nBUSINESS VALUE PROPOSITION\n\n\n\n\n\n\n\n\nOver 50 leading tech partners have adopted A2A allowing AI agents to exchange information across platforms.\nA2A  connects AI agents across different departments and cloud environments, breaking down traditional silos and enabling unified workflows.\nOrganizations can build flexible, modular AI ecosystems where specialized agents can be added, upgraded, or replaced\n\nThis allows organizations to rapidly deploy new agents, with minimal integration costs.\nAI agents where first-line support bots handle general inquiries and escalate specialized issues to expert agents.\u00a0\nAgents can also hand off cases across time zones to collaborate with human representatives as needed.\nCustomer support, recommendation  and delivery agents can coordinate to provide instant, personalized order fulfillment, and multilingual support.\nImproved customer satisfaction and reduced operational costs.\nVendor-Agnostic Workflows\nCustomer Service Automation\nModular AI Infrastructure\nRetail and E-Commerce\nMCP and A2A are not competing, but complementary protocols.\nMCP enables tool-level integration, while A2A enables multi-agent collaboration.\nWhile MCP enables communication with tools, A2A can manage coordination and task distribution among agents.\nTogether, they form the backbone of modular, pluggable, AI systems.\nA2A agents will be able to reason, plan, and negotiate across tools registered via MCP..\nThis enables the development of self-improving agent systems.\nMCP and A2A driven systems can lowers the barrier to entry for integrating AI into enterprise workflows.\nSets the stage for non-technical teams to participate in the AI integration process.\nCONTROL FLOW FOR OUR MOCK MCP SERVER PROJECT\nCHALLENGES WITH LARGE CONTEXTS AND STRUCTURED DATA\nCURRENT LIMITATIONS\nPROPOSED SOLUTION\nWorks across industries, including finance, supply chain, HR, policy, compliance\nEnables natural interaction with complex systems \u2014 no query language needed\nCombines memory, reasoning, and explainability\nArchitecture-layer innovation \u2014 extensible to any LLM (e.g., GPT, Claude, LLaMA)\nIdeal foundation for intelligent enterprise copilots and audit-ready agents\n\n--- SPEECH.docx ---\nSLIDE 1\nAs most of you already know, Large Language Models are constrained by fixed context windows. Whether it's 4K, 16K, or even 100K tokens \u2014 if the input is too long, older parts of the conversation get truncated, leading to loss of memory and coherence.\nIn enterprise settings, this becomes especially problematic \u2014 because client conversations, audit logs, or case threads are often large, multi-turn, and full of nested meaning.\nThe second challenge Is that  LLMs aren\u2019t naturally built to traverse nested structures like JSON data in APIs. They understand text, but not hierarchies unless explicitly guided.\nHow can we make LLMs remember and reason better \u2014 across long histories and structured information \u2014 without changing the model itself?\u201d\n\u2705 Language models like GPT, Claude, and LLaMA are fundamentally capable of semantic reasoning.\nThey understand meanings, synonyms, intent, and even latent relationships between concepts \u2014 all through learned embeddings and their internal attention mechanisms.\nBut \u2014 and this is key \u2014 that semantic reasoning is largely passive. It enables them to understand language well, but not to act on it strategically in external systems.\n\u274c LLMs don\u2019t naturally know how to traverse external APIs, navigate nested JSON structures, or explore live data schemas based on a goal.\n\n\u274c Likewise, when dealing with long histories \u2014 like ongoing chats or logs \u2014 LLMs don\u2019t know which parts are most important for the current query.\nThey\u2019ll either take in too much (and get confused), or truncate blindly based on token limits, losing valuable information.\nSo this calls for a 2-part solution.\nThe first part is called Dynamic Context Compression:\nIt intelligently breaks a long chat history into manageable chunks\nThen it ranks those chunks by semantic relevance to the current question using TF-IDF\nAnd it uses the LLM to summarize less-relevant parts, so we can stay within the token limit without losing memory.\nThe second part is called Semantic Navigation:\nWhen given a complex JSON or API response, the system prompts the LLM to choose which key to explore next, based on the user\u2019s intent.\nIt doesn\u2019t rely on schema knowledge or hardcoded logic.\nInstead, it uses semantic inference to traverse hierarchies \u2014 dynamically \u2014 until it finds the right value.\nThese two components together allow LLMs to act as intelligent, memory-aware, structure-savvy agents \u2014 while keeping everything model-agnostic and extensible.\u201d\nSLIDE 2\n SEMANTIC NAVIGATION WORKFLOW\n\u201cLet me walk you through the Semantic Navigation workflow, because this is where things get interesting.\nThe user enters a query \u2014 for example: \u201cWhat is the capital of the country with the highest population?\u201d\nThe system passes this query and the current JSON structure to the LLM.\nThe LLM analyzes the available keys, and semantically chooses the most relevant one \u2014 not by string-matching, but by understanding latent meaning.\nFor example, it may correctly infer that \u2018main_office_location\u2019 is equivalent to \u2018capital\u2019.\nIf the value is itself another dict or a list, the system recursively prompts the LLM to choose again.\nThis continues until the final value is reached \u2014 and the system returns the answer with a traceable path of how it got there.\nSo we\u2019re not just getting answers \u2014 we\u2019re getting a reasoning trail.\u201d\nSLIDE 3\nNow let\u2019s zoom out and look at the bigger picture.\nThis approach is domain-agnostic \u2014 it can be used in finance to explore transaction logs, in supply chains to analyze JSON API flows, or in HR to track policy Q&A logs.\nThe UI is simple \u2014 natural language. No query language. No schema awareness.\nAnd the model remains untouched \u2014 we\u2019ve added value entirely at the architecture level, so it works with GPT, Claude, LLaMA, or any model we deploy internally.\nThat\u2019s the key takeaway here:\n\u2705 We\u2019ve taken what LLMs already do \u2014 semantic understanding \u2014\n\ud83d\udd01 And turned it into a system that adds semantic planning, navigation, memory compression, and traceability.\nThis is the kind of modular, composable innovation that will power the next wave of enterprise copilots \u2014 and we\u2019ve built the blueprint for it.\u201d\nCODE\n\ud83d\uddc2\ufe0f 1. app.py \u2014 The Frontend Controller\n\u201cThis is the main Streamlit app. It ties everything together into a usable interface.\nUsers can enter a query, adjust the compression budget, and either run a standard LLM query or trigger semantic navigation.\nIt reads from and writes to sample_chat.json, maintaining multi-turn history.\nIt also integrates with the external RestCountries API and sends individual country data into the Semantic Navigator for reasoning.\u201d\n\ud83e\udde0 2. compressor.py \u2014 Dynamic Context Compression Engine\n\u201cThis file handles dynamic context compression.\nIt breaks the full chat history into token-sized chunks, ranks them based on how relevant they are to the current query using TF-IDF, and summarizes the lower-priority ones using the LLM.\nThis ensures we always stay within the model\u2019s token limit \u2014 without losing key context.\u201d\n\ud83d\udd0c 3. llama_api.py \u2014 LLM Request Handler\n\u201cThis is the API interface to our local LLaMA model served via Ollama.\nIt sends prompts, receives responses, and handles generation parameters like max tokens and temperature.\nIt keeps the entire solution model-agnostic and deployable locally \u2014 which is critical for privacy-focused or regulated clients.\u201d\n\ud83e\uddfe 4. sample_chat.json \u2014 Memory Log\n\u201cThis is the chat memory file. Every query and response is stored here.\nIt allows the system to simulate memory across turns and ensures that context compression always operates on the full historical thread.\nIt's what makes our LLM \u2018aware\u2019 of past interactions in a long conversation.\u201d\n\ud83e\udded 5. semantic_navigator.py \u2014 Semantic JSON Explorer\n\u201cThis is the core semantic navigation module.\nIt takes any nested JSON and, at each level, prompts the LLM to pick the most semantically relevant key \u2014 not through string matching, but by reasoning about meaning.\nIt recursively follows that path, building a trace as it goes, until it reaches a final value.\nThis gives the LLM a way to explore unknown data structures \u2014 like APIs or documents \u2014 step by step.\u201d\n\n--- activity_logger.py (PY) ---\nimport win32gui\nimport win32process\nimport psutil\nimport time\nimport json\nfrom datetime import datetime\n\nLOG_FILE = \"activity_log.jsonl\"\n\ndef get_active_window_info():\n    try:\n        hwnd = win32gui.GetForegroundWindow()\n        _, pid = win32process.GetWindowThreadProcessId(hwnd)\n        process = psutil.Process(pid)\n        window_title = win32gui.GetWindowText(hwnd)\n        app_name = process.name()\n        return {\n            \"timestamp\": datetime.now().isoformat(),\n            \"app\": app_name,\n            \"title\": window_title\n        }\n    except Exception as e:\n        return {\n            \"timestamp\": datetime.now().isoformat(),\n            \"app\": \"Unknown\",\n            \"title\": f\"Error: {str(e)}\"\n        }\n\ndef log_activity():\n    last_app = None\n    print(\"\ud83d\udfe2 ContextPilot Activity Logger started. Press Ctrl+C to stop.\\n\")\n    while True:\n        info = get_active_window_info()\n        if info[\"app\"] != last_app:  # Log only when app changes\n            print(f\"[{info['timestamp']}] {info['app']} - {info['title']}\")\n            with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n                f.write(json.dumps(info) + \"\\n\")\n            last_app = info[\"app\"]\n        time.sleep(5)  # Check every 5 seconds\n\nif __name__ == \"__main__\":\n    log_activity()\n\n\n--- capsule_builder.py (PY) ---\nimport os\nimport json\nimport requests\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Configuration\nGROQ_API_KEY = \"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\"\nGROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\"\nGROQ_MODEL = \"meta-llama/llama-4-scout-17b-16e-instruct\"\nSUPPORTED_EXTENSIONS = [\".docx\", \".txt\", \".md\"]\nOUTPUT_DIR = \"capsules\"\n\n# Create capsule directory if not exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef find_latest_supported_file():\n    files = [f for f in Path(\".\").glob(\"*\") if f.suffix.lower() in SUPPORTED_EXTENSIONS]\n    if not files:\n        raise FileNotFoundError(\"No supported .docx/.txt/.md files found in this folder.\")\n    return max(files, key=lambda f: f.stat().st_mtime)\n\ndef read_input_file(file_path):\n    ext = file_path.suffix.lower()\n    if ext == \".docx\":\n        import docx\n        doc = docx.Document(file_path)\n        return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n    elif ext in [\".txt\", \".md\"]:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n    else:\n        raise ValueError(f\"Unsupported file format: {ext}\")\n\ndef summarize_context(context_text):\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {GROQ_API_KEY}\"\n    }\n\n    system_prompt = (\n        \"You are a summarization assistant. Read the session context below and summarize the user's goals and actions in 3 sentences.\"\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": context_text}\n    ]\n\n    response = requests.post(GROQ_API_URL, headers=headers, json={\"model\": GROQ_MODEL, \"messages\": messages})\n    if response.status_code == 200:\n        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n    else:\n        raise RuntimeError(f\"Groq API error {response.status_code}: {response.text}\")\n\ndef save_capsule(context, summary):\n    timestamp = datetime.now().strftime(\"%Y-%m-%dT%H-%M\")\n    capsule_path = Path(OUTPUT_DIR) / f\"capsule_{timestamp}.json\"\n\n    with open(capsule_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"context\": context, \"summary\": summary}, f, indent=2)\n\n    print(f\"\\n\u2705 Capsule saved to: {capsule_path}\")\n\nif __name__ == \"__main__\":\n    try:\n        latest_file = find_latest_supported_file()\n        print(f\"\ud83d\udcc2 Using latest file: {latest_file.name}\")\n        context = read_input_file(latest_file)\n        print(\"\u270f\ufe0f Summarizing content via Groq...\")\n        summary = summarize_context(context)\n        print(\"\\n\ud83e\udde0 Summary:\")\n        print(summary)\n        save_capsule(context, summary)\n    except Exception as e:\n        print(f\"\u274c Error: {e}\")\n\n\n--- capsule_builder2.py (PY) ---\nimport os\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom pptx import Presentation\nimport docx\nimport requests\n\nACTIVITY_LOG_FILE = \"activity_log.jsonl\"\nCAPSULE_DIR = \"capsules\"\nGROQ_API_KEY = \"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\"\nGROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\"\nGROQ_MODEL = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n\nPath(CAPSULE_DIR).mkdir(exist_ok=True)\n\ndef load_activity_log():\n    activities = []\n    if os.path.exists(ACTIVITY_LOG_FILE):\n        with open(ACTIVITY_LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                try:\n                    entry = json.loads(line.strip())\n                    if entry.get(\"app\") and entry.get(\"title\"):\n                        activities.append(entry)\n                except json.JSONDecodeError:\n                    continue\n    return activities\n\ndef extract_code_files():\n    code_files = []\n    for file in os.listdir(\".\"):\n        if file.endswith(\".py\"):\n            with open(file, \"r\", encoding=\"utf-8\") as f:\n                code = f.read()\n                code_files.append(f\"--- {file} (PY) ---\\n{code}\")\n    return \"\\n\\n\".join(code_files)\n\ndef extract_word_content(filename):\n    try:\n        doc = docx.Document(filename)\n        return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n    except Exception as e:\n        return f\"[Error reading {filename}]: {str(e)}\"\n\ndef extract_ppt_content(filename):\n    try:\n        prs = Presentation(filename)\n        text_runs = []\n        for slide in prs.slides:\n            for shape in slide.shapes:\n                if hasattr(shape, \"text\"):\n                    text_runs.append(shape.text.strip())\n        return \"\\n\".join(text_runs)\n    except Exception as e:\n        return f\"[Error reading {filename}]: {str(e)}\"\n\ndef summarize_context(context_text):\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {GROQ_API_KEY}\"\n    }\n\n    system_prompt = (\n        \"You are a summarization assistant. Read the session context below and summarize the user's goals and actions in 3 sentences.\"\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": context_text}\n    ]\n\n    response = requests.post(GROQ_API_URL, headers=headers, json={\"model\": GROQ_MODEL, \"messages\": messages})\n    if response.status_code == 200:\n        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n    else:\n        return f\"[Groq API error {response.status_code}]: {response.text}\"\n\ndef save_capsule(context, summary):\n    timestamp = datetime.now().strftime(\"%Y-%m-%dT%H-%M\")\n    capsule_path = Path(CAPSULE_DIR) / f\"capsule_{timestamp}.json\"\n    with open(capsule_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"context\": context, \"summary\": summary}, f, indent=2)\n    print(f\"\\n\u2705 Capsule saved: {capsule_path}\")\n\nif __name__ == \"__main__\":\n    print(\"\ud83e\udde0 Building new capsule with Word + PPT + Logs + Code...\\n\")\n\n    # Load activities\n    activities = load_activity_log()\n    activity_summary = \"\"\n    file_contexts = []\n\n    for entry in activities:\n        app = entry[\"app\"]\n        title = entry[\"title\"]\n        activity_summary += f\"- [{entry['timestamp']}] Opened `{title}` using `{app}`\\n\"\n\n        # Parse Word file if title matches\n        if \"Word\" in title and \"SPEECH\" in title:\n            file_contexts.append(f\"--- SPEECH.docx ---\\n{extract_word_content('SPEECH.docx')}\")\n        elif \"PowerPoint\" in title and \"MCP_A2A\" in title:\n            file_contexts.append(f\"--- MCP_A2A.pptx ---\\n{extract_ppt_content('MCP_A2A.pptx')}\")\n\n    # Add code files\n    code_block = extract_code_files()\n    full_context = f\"Here is an activity summary of the user's session:\\n{activity_summary}\\n\\n\" + \"\\n\\n\".join(file_contexts) + \"\\n\\n\" + code_block\n\n    # Summarize + Save\n    print(\"\ud83d\udcdd Summary:\")\n    summary = summarize_context(full_context)\n    print(\"\", summary)\n    save_capsule(full_context, summary)\n\n\n--- resume.py (PY) ---\nimport json\nimport os\nfrom autogen import AssistantAgent, UserProxyAgent\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Set the OpenAI key in code (as requested)\nos.environ[\"OPENAI_API_KEY\"] = \"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\"\n\nCAPSULE_DIR = \"capsules\"\nLATEST_CAPSULE = sorted(Path(CAPSULE_DIR).glob(\"capsule_*.json\"))[-1]\n\ndef load_latest_capsule():\n    with open(LATEST_CAPSULE, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef run_query_on_capsule(context_text, user_question):\n    from autogen.oai.openai_utils import config_list_from_json\n\n    assistant = AssistantAgent(\n        name=\"resume_agent\",\n        system_message=\"You are a workplace assistant. Based on the following session context, answer the user's questions.\",\n        llm_config={\n            \"config_list\": [\n                {\n                    \"model\": \"gpt-4\",\n                    \"api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n                }\n            ],\n            \"model\": \"gpt-4\",\n            \"temperature\": 0.7,\n            \"stream\": False,\n        },\n    )\n\n    user = UserProxyAgent(\n        name=\"user\",\n        human_input_mode=\"NEVER\",\n        is_termination_msg=lambda x: isinstance(x, str) and x.strip().lower() in {\"exit\", \"quit\"},\n    )\n\n    user.send(\n        f\"Here is a summary of your last session:\\n\\n{context_text}\\n\\nNow answer this: {user_question}\",\n        assistant\n    )\n\n    user.initiate_chat(assistant, message=user_question, max_turns=1)\n\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd01 Resuming last context capsule...\\n\")\n\n    capsule = load_latest_capsule()\n    print(f\"\ud83d\udcc2 Loaded capsule from: {LATEST_CAPSULE}\")\n    print(f\"\\n\ud83e\udde0 Capsule Summary:\\n{capsule.get('summary', '(No summary)')}\\n\")\n    print(f\"\ud83d\udcc4 Context Size: {len(capsule.get('context', '')) // 4} tokens (approx.)\\n\")\n\n    question = input(\"> What would you like to ask the assistant about this session?\\n> \").strip()\n    if question:\n        run_query_on_capsule(capsule[\"context\"], question)\n\n\n--- resume_session.py (PY) ---\nimport json\nfrom autogen import AssistantAgent, UserProxyAgent\nfrom pathlib import Path\n\nCAPSULE_DIR = \"capsules\"\nLATEST_CAPSULE = sorted(Path(CAPSULE_DIR).glob(\"capsule_*.json\"))[-1]\n\ndef load_latest_capsule():\n    with open(LATEST_CAPSULE, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef run_query_on_capsule(context_text, user_question):\n    assistant = AssistantAgent(\n        name=\"resume_agent\",\n        system_message=\"You are a workplace assistant. Answer the user's question based on the session content below.\",\n        llm_config={\n            \"config_list\": [\n                {\n                    \"model\": \"llama3.2:latest\",\n                    \"base_url\": \"http://localhost:11434/v1\",\n                    \"api_key\": \"ollama\",  # required dummy\n                    \"price\": [0, 0],      # optional for suppressing cost warning\n                }\n            ],\n            \"model\": \"llama3.2:latest\",\n            \"temperature\": 0.5,\n            \"stream\": False,\n        },\n    )\n\n    user = UserProxyAgent(\n        name=\"user\",\n        human_input_mode=\"NEVER\",\n        is_termination_msg=lambda x: isinstance(x, str) and x.strip().lower() in {\"exit\", \"quit\"},\n    )\n\n    # First: give the assistant the document content\n    user.send(f\"\"\"\nYou were helping me work on a Word document titled \"SPEECH.docx\". Here is the extracted content:\n\n{context_text}\n\"\"\", assistant)\n\n    # Second: ask the question\n    user.initiate_chat(assistant, message=\"Summarize what I did in the Word document.\", max_turns=1)\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd01 Resuming last context capsule...\\n\")\n    capsule = load_latest_capsule()\n\n    print(f\"\ud83d\udcc2 Loaded capsule from: {LATEST_CAPSULE}\")\n    print(f\"\\n\ud83e\udde0 Capsule Summary:\\n{capsule.get('summary', '(No summary)')}\\n\")\n    print(f\"\ud83d\udcc4 Context Size: {len(capsule.get('context', '')) // 4} tokens (approx.)\\n\")\n\n    run_query_on_capsule(capsule[\"context\"], \"Summarize what I did in the Word document.\")\n\n\n--- resume_session_gpt35.py (PY) ---\n\nimport json\nimport os\nfrom autogen import AssistantAgent, UserProxyAgent\nfrom datetime import datetime\nfrom pathlib import Path\n\nCAPSULE_DIR = \"capsules\"\nLATEST_CAPSULE = sorted(Path(CAPSULE_DIR).glob(\"capsule_*.json\"))[-1]\n\ndef load_latest_capsule():\n    with open(LATEST_CAPSULE, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef run_query_on_capsule(context_text, user_question):\n    from autogen.oai.openai_utils import config_list_from_json\n\n    assistant = AssistantAgent(\n        name=\"resume_agent\",\n        system_message=\"You are a workplace assistant. Based on the following session context, answer the user's questions.\",\n        llm_config={\n            \"config_list\": [\n                {\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"api_key\": \"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\"\n                }\n            ],\n            \"model\": \"gpt-3.5-turbo\",\n            \"temperature\": 0.7,\n            \"stream\": False,\n        },\n    )\n\n    user = UserProxyAgent(\n        name=\"user\",\n        human_input_mode=\"NEVER\",\n        is_termination_msg=lambda x: isinstance(x, str) and x.strip().lower() in {\"exit\", \"quit\"},\n    )\n\n    user.send(\n        f\"Here is a summary of your last session:\\n\\n{context_text}\\n\\nNow answer this: {user_question}\",\n        assistant\n    )\n\n    user.initiate_chat(assistant, message=user_question, max_turns=1)\n\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd01 Resuming last context capsule...\\n\")\n\n    capsule = load_latest_capsule()\n    print(f\"\ud83d\udcc2 Loaded capsule from: {LATEST_CAPSULE}\")\n    print(f\"\\n\ud83e\udde0 Capsule Summary:\\n{capsule.get('summary', '(No summary)')}\\n\")\n    print(f\"\ud83d\udcc4 Context Size: {len(capsule.get('context', '')) // 4} tokens (approx.)\\n\")\n\n    question = input(\"> What would you like to ask the assistant about this session?\\n> \").strip()\n    if question:\n        run_query_on_capsule(capsule[\"context\"], question)\n\n\n--- resume_session_groq.py (PY) ---\nimport json\nimport requests\nfrom datetime import datetime\nfrom pathlib import Path\n\nCAPSULE_DIR = \"capsules\"\nLATEST_CAPSULE = sorted(Path(CAPSULE_DIR).glob(\"capsule_*.json\"))[-1]\n\nGROQ_API_KEY = \"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\"\nGROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\"\nGROQ_MODEL = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n\ndef load_latest_capsule():\n    with open(LATEST_CAPSULE, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef query_groq_model(context_text, user_question):\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    }\n\n    prompt = f\"\"\"You are a workplace assistant.\n\nHere is the session context:\n{context_text}\n\nNow answer this question from the user:\n{user_question}\n\"\"\"\n\n    data = {\n        \"model\": GROQ_MODEL,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    }\n\n    response = requests.post(GROQ_API_URL, headers=headers, json=data)\n    if response.status_code == 200:\n        content = response.json()\n        print(\"\\n\ud83e\udd16 Assistant Response:\")\n        print(content[\"choices\"][0][\"message\"][\"content\"])\n    else:\n        print(f\"\u274c Error {response.status_code}: {response.text}\")\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd01 Resuming last context capsule...\\n\")\n\n    capsule = load_latest_capsule()\n    print(f\"\ud83d\udcc2 Loaded capsule from: {LATEST_CAPSULE}\")\n    print(f\"\\n\ud83e\udde0 Capsule Summary:\\n{capsule.get('summary', '(No summary)')}\\n\")\n    print(f\"\ud83d\udcc4 Context Size: {len(capsule.get('context', '')) // 4} tokens (approx.)\\n\")\n\n    question = input(\"> What would you like to ask the assistant about this session?\\n> \").strip()\n    if question:\n        query_groq_model(capsule[\"context\"], question)\n",
  "summary": "Here is a summary of the user's goals and actions in 3 sentences:\n\nThe user opened a PowerPoint file titled \"MCP_A2A - PowerPoint\" and a Word document titled \"SPEECH.docx\" to work on a project related to Large Language Models (LLMs) and their limitations in business contexts. The user's goals appear to be centered around understanding the strengths and current use cases of LLMs, as well as overcoming their shortcomings using augmentation. The user also worked with various Python scripts, including capsule_builder.py and resume.py, to build and resume a context capsule summarizing their session."
}